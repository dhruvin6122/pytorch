{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQ2Fon3MAEisqgU1Y0R+Hx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvin6122/pytorch/blob/main/Simple_Ann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AATCKrgZR_VK",
        "outputId": "ce90a2c9-ae10-48f5-ee04-20eaf03568b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.8.0+cpu\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# -----------------------\n",
        "# 1. Load dataset (digits 8x8 images -> 64 features)\n",
        "# -----------------------\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader\n",
        "train_ds = TensorDataset(X_train, y_train)\n",
        "test_ds = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=32)\n",
        "\n",
        "# -----------------------\n",
        "# 2. Define ANN Model\n",
        "# -----------------------\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(ANN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)   # first hidden layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  # output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Model instance\n",
        "model = ANN(input_size=64, hidden_size=32, num_classes=10)\n",
        "\n",
        "# -----------------------\n",
        "# 3. Loss & Optimizer\n",
        "# -----------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "# -----------------------\n",
        "# 4. Training Loop\n",
        "# -----------------------\n",
        "for epoch in range(200):  # 20 epochs\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "\n",
        "        # Backward & optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/20], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# -----------------------\n",
        "# 5. Evaluation\n",
        "# -----------------------\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        outputs = model(X_batch)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test data: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9pIWU5oUKba",
        "outputId": "d07c7f2c-0362-48ab-d908-495fbe88f9dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 1.7628\n",
            "Epoch [2/20], Loss: 1.0897\n",
            "Epoch [3/20], Loss: 0.7418\n",
            "Epoch [4/20], Loss: 0.5674\n",
            "Epoch [5/20], Loss: 0.2736\n",
            "Epoch [6/20], Loss: 0.2317\n",
            "Epoch [7/20], Loss: 0.2187\n",
            "Epoch [8/20], Loss: 0.1879\n",
            "Epoch [9/20], Loss: 0.1483\n",
            "Epoch [10/20], Loss: 0.1963\n",
            "Epoch [11/20], Loss: 0.0963\n",
            "Epoch [12/20], Loss: 0.2936\n",
            "Epoch [13/20], Loss: 0.1092\n",
            "Epoch [14/20], Loss: 0.1456\n",
            "Epoch [15/20], Loss: 0.0601\n",
            "Epoch [16/20], Loss: 0.0409\n",
            "Epoch [17/20], Loss: 0.0455\n",
            "Epoch [18/20], Loss: 0.0754\n",
            "Epoch [19/20], Loss: 0.0275\n",
            "Epoch [20/20], Loss: 0.0622\n",
            "Epoch [21/20], Loss: 0.0148\n",
            "Epoch [22/20], Loss: 0.0248\n",
            "Epoch [23/20], Loss: 0.0914\n",
            "Epoch [24/20], Loss: 0.0281\n",
            "Epoch [25/20], Loss: 0.0361\n",
            "Epoch [26/20], Loss: 0.0313\n",
            "Epoch [27/20], Loss: 0.0416\n",
            "Epoch [28/20], Loss: 0.0804\n",
            "Epoch [29/20], Loss: 0.0266\n",
            "Epoch [30/20], Loss: 0.0130\n",
            "Epoch [31/20], Loss: 0.0136\n",
            "Epoch [32/20], Loss: 0.0177\n",
            "Epoch [33/20], Loss: 0.0104\n",
            "Epoch [34/20], Loss: 0.0147\n",
            "Epoch [35/20], Loss: 0.0242\n",
            "Epoch [36/20], Loss: 0.0038\n",
            "Epoch [37/20], Loss: 0.0032\n",
            "Epoch [38/20], Loss: 0.0127\n",
            "Epoch [39/20], Loss: 0.0074\n",
            "Epoch [40/20], Loss: 0.0140\n",
            "Epoch [41/20], Loss: 0.0051\n",
            "Epoch [42/20], Loss: 0.0100\n",
            "Epoch [43/20], Loss: 0.0194\n",
            "Epoch [44/20], Loss: 0.0152\n",
            "Epoch [45/20], Loss: 0.0049\n",
            "Epoch [46/20], Loss: 0.0068\n",
            "Epoch [47/20], Loss: 0.0241\n",
            "Epoch [48/20], Loss: 0.0079\n",
            "Epoch [49/20], Loss: 0.0059\n",
            "Epoch [50/20], Loss: 0.0118\n",
            "Epoch [51/20], Loss: 0.0072\n",
            "Epoch [52/20], Loss: 0.0112\n",
            "Epoch [53/20], Loss: 0.0050\n",
            "Epoch [54/20], Loss: 0.0099\n",
            "Epoch [55/20], Loss: 0.0080\n",
            "Epoch [56/20], Loss: 0.0019\n",
            "Epoch [57/20], Loss: 0.0047\n",
            "Epoch [58/20], Loss: 0.0068\n",
            "Epoch [59/20], Loss: 0.0070\n",
            "Epoch [60/20], Loss: 0.0036\n",
            "Epoch [61/20], Loss: 0.0074\n",
            "Epoch [62/20], Loss: 0.0057\n",
            "Epoch [63/20], Loss: 0.0035\n",
            "Epoch [64/20], Loss: 0.0093\n",
            "Epoch [65/20], Loss: 0.0129\n",
            "Epoch [66/20], Loss: 0.0030\n",
            "Epoch [67/20], Loss: 0.0015\n",
            "Epoch [68/20], Loss: 0.0119\n",
            "Epoch [69/20], Loss: 0.0034\n",
            "Epoch [70/20], Loss: 0.0024\n",
            "Epoch [71/20], Loss: 0.0079\n",
            "Epoch [72/20], Loss: 0.0014\n",
            "Epoch [73/20], Loss: 0.0013\n",
            "Epoch [74/20], Loss: 0.0016\n",
            "Epoch [75/20], Loss: 0.0022\n",
            "Epoch [76/20], Loss: 0.0039\n",
            "Epoch [77/20], Loss: 0.0015\n",
            "Epoch [78/20], Loss: 0.0022\n",
            "Epoch [79/20], Loss: 0.0011\n",
            "Epoch [80/20], Loss: 0.0018\n",
            "Epoch [81/20], Loss: 0.0013\n",
            "Epoch [82/20], Loss: 0.0020\n",
            "Epoch [83/20], Loss: 0.0010\n",
            "Epoch [84/20], Loss: 0.0011\n",
            "Epoch [85/20], Loss: 0.0043\n",
            "Epoch [86/20], Loss: 0.0018\n",
            "Epoch [87/20], Loss: 0.0040\n",
            "Epoch [88/20], Loss: 0.0006\n",
            "Epoch [89/20], Loss: 0.0007\n",
            "Epoch [90/20], Loss: 0.0010\n",
            "Epoch [91/20], Loss: 0.0027\n",
            "Epoch [92/20], Loss: 0.0016\n",
            "Epoch [93/20], Loss: 0.0009\n",
            "Epoch [94/20], Loss: 0.0018\n",
            "Epoch [95/20], Loss: 0.0022\n",
            "Epoch [96/20], Loss: 0.0018\n",
            "Epoch [97/20], Loss: 0.0027\n",
            "Epoch [98/20], Loss: 0.0014\n",
            "Epoch [99/20], Loss: 0.0025\n",
            "Epoch [100/20], Loss: 0.0031\n",
            "Epoch [101/20], Loss: 0.0016\n",
            "Epoch [102/20], Loss: 0.0003\n",
            "Epoch [103/20], Loss: 0.0003\n",
            "Epoch [104/20], Loss: 0.0022\n",
            "Epoch [105/20], Loss: 0.0014\n",
            "Epoch [106/20], Loss: 0.0018\n",
            "Epoch [107/20], Loss: 0.0008\n",
            "Epoch [108/20], Loss: 0.0014\n",
            "Epoch [109/20], Loss: 0.0014\n",
            "Epoch [110/20], Loss: 0.0002\n",
            "Epoch [111/20], Loss: 0.0008\n",
            "Epoch [112/20], Loss: 0.0006\n",
            "Epoch [113/20], Loss: 0.0006\n",
            "Epoch [114/20], Loss: 0.0004\n",
            "Epoch [115/20], Loss: 0.0002\n",
            "Epoch [116/20], Loss: 0.0005\n",
            "Epoch [117/20], Loss: 0.0019\n",
            "Epoch [118/20], Loss: 0.0004\n",
            "Epoch [119/20], Loss: 0.0005\n",
            "Epoch [120/20], Loss: 0.0003\n",
            "Epoch [121/20], Loss: 0.0006\n",
            "Epoch [122/20], Loss: 0.0003\n",
            "Epoch [123/20], Loss: 0.0005\n",
            "Epoch [124/20], Loss: 0.0009\n",
            "Epoch [125/20], Loss: 0.0006\n",
            "Epoch [126/20], Loss: 0.0005\n",
            "Epoch [127/20], Loss: 0.0010\n",
            "Epoch [128/20], Loss: 0.0005\n",
            "Epoch [129/20], Loss: 0.0001\n",
            "Epoch [130/20], Loss: 0.0006\n",
            "Epoch [131/20], Loss: 0.0002\n",
            "Epoch [132/20], Loss: 0.0006\n",
            "Epoch [133/20], Loss: 0.0004\n",
            "Epoch [134/20], Loss: 0.0007\n",
            "Epoch [135/20], Loss: 0.0002\n",
            "Epoch [136/20], Loss: 0.0006\n",
            "Epoch [137/20], Loss: 0.0008\n",
            "Epoch [138/20], Loss: 0.0002\n",
            "Epoch [139/20], Loss: 0.0004\n",
            "Epoch [140/20], Loss: 0.0002\n",
            "Epoch [141/20], Loss: 0.0002\n",
            "Epoch [142/20], Loss: 0.0007\n",
            "Epoch [143/20], Loss: 0.0002\n",
            "Epoch [144/20], Loss: 0.0001\n",
            "Epoch [145/20], Loss: 0.0001\n",
            "Epoch [146/20], Loss: 0.0002\n",
            "Epoch [147/20], Loss: 0.0003\n",
            "Epoch [148/20], Loss: 0.0003\n",
            "Epoch [149/20], Loss: 0.0003\n",
            "Epoch [150/20], Loss: 0.0003\n",
            "Epoch [151/20], Loss: 0.0005\n",
            "Epoch [152/20], Loss: 0.0004\n",
            "Epoch [153/20], Loss: 0.0001\n",
            "Epoch [154/20], Loss: 0.0004\n",
            "Epoch [155/20], Loss: 0.0002\n",
            "Epoch [156/20], Loss: 0.0005\n",
            "Epoch [157/20], Loss: 0.0003\n",
            "Epoch [158/20], Loss: 0.0003\n",
            "Epoch [159/20], Loss: 0.0002\n",
            "Epoch [160/20], Loss: 0.0003\n",
            "Epoch [161/20], Loss: 0.0002\n",
            "Epoch [162/20], Loss: 0.0002\n",
            "Epoch [163/20], Loss: 0.0003\n",
            "Epoch [164/20], Loss: 0.0001\n",
            "Epoch [165/20], Loss: 0.0000\n",
            "Epoch [166/20], Loss: 0.0002\n",
            "Epoch [167/20], Loss: 0.0001\n",
            "Epoch [168/20], Loss: 0.0002\n",
            "Epoch [169/20], Loss: 0.0002\n",
            "Epoch [170/20], Loss: 0.0001\n",
            "Epoch [171/20], Loss: 0.0001\n",
            "Epoch [172/20], Loss: 0.0001\n",
            "Epoch [173/20], Loss: 0.0001\n",
            "Epoch [174/20], Loss: 0.0002\n",
            "Epoch [175/20], Loss: 0.0002\n",
            "Epoch [176/20], Loss: 0.0002\n",
            "Epoch [177/20], Loss: 0.0001\n",
            "Epoch [178/20], Loss: 0.0002\n",
            "Epoch [179/20], Loss: 0.0001\n",
            "Epoch [180/20], Loss: 0.0001\n",
            "Epoch [181/20], Loss: 0.0001\n",
            "Epoch [182/20], Loss: 0.0002\n",
            "Epoch [183/20], Loss: 0.0001\n",
            "Epoch [184/20], Loss: 0.0001\n",
            "Epoch [185/20], Loss: 0.0002\n",
            "Epoch [186/20], Loss: 0.0001\n",
            "Epoch [187/20], Loss: 0.0001\n",
            "Epoch [188/20], Loss: 0.0000\n",
            "Epoch [189/20], Loss: 0.0001\n",
            "Epoch [190/20], Loss: 0.0001\n",
            "Epoch [191/20], Loss: 0.0001\n",
            "Epoch [192/20], Loss: 0.0002\n",
            "Epoch [193/20], Loss: 0.0002\n",
            "Epoch [194/20], Loss: 0.0001\n",
            "Epoch [195/20], Loss: 0.0000\n",
            "Epoch [196/20], Loss: 0.0001\n",
            "Epoch [197/20], Loss: 0.0001\n",
            "Epoch [198/20], Loss: 0.0000\n",
            "Epoch [199/20], Loss: 0.0000\n",
            "Epoch [200/20], Loss: 0.0001\n",
            "Accuracy on test data: 97.78%\n"
          ]
        }
      ]
    }
  ]
}